{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95fe2cc6-1751-41f7-8987-c3cd87e4656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import norm\n",
    "\n",
    "from data_generation import m_0, g_0, get_data\n",
    "#from dml_algorithm import mm_ate, dml_ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd598b24-4e98-4412-aa4e-63ac7ff2e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_simulation(N, model_g, model_m, n_MC=5000):\n",
    "    np.random.seed(100)\n",
    "    ate_estimates = np.empty((n_MC, 4))\n",
    "    sigma_estimates = np.empty(n_MC)\n",
    "    CIs = np.empty((n_MC, 2))\n",
    "\n",
    "    for j in range(n_MC):\n",
    "        y_data, d_data, x_data = get_data(N)\n",
    "        ate_estimates[j, 0] = mm_ate(y_data, d_data, x_data, g_0, m_0)\n",
    "        ate_estimates[j, 1:], sigma_estimates[j], CIs[j] = dml_ate(y_data, d_data, x_data, model_g, model_m)\n",
    "\n",
    "    return [ate_estimates, sigma_estimates, CIs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bee25123-cb5d-4b2b-b042-1a4fd1026d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dml_ate(y_data, d_data, x_data, model_g, model_m, K=5, classical=True, inference=True, alpha=0.05):\n",
    "    # Generate random partition of data for cross-fitting\n",
    "    N = len(y_data)\n",
    "    skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "    # Compute respective ML estimators and thereupon auxiliary estimators\n",
    "    theta_0_check_list = []\n",
    "    if classical:\n",
    "        reg_check_list, ipw_check_list = [], []\n",
    "    if inference:\n",
    "        scores_list = []\n",
    "    \n",
    "    for (train_indices, eval_indices) in skf.split(X=x_data, y=d_data):\n",
    "        y_train, d_train, x_train = y_data[train_indices], d_data[train_indices], x_data[train_indices]\n",
    "        y_eval, d_eval, x_eval = y_data[eval_indices], d_data[eval_indices], x_data[eval_indices]\n",
    "\n",
    "        # Estimate outcome regression functions g_0(d)\n",
    "        g_0_hat = []\n",
    "        model_g0 = Sequential()\n",
    "        model_g0.add(Input(shape=(3,)))\n",
    "        model_g0.add(Dense(10, activation='relu'))\n",
    "        model_g0.add(Dropout(0.2))\n",
    "        model_g0.add(Dense(1))\n",
    "        model_g0.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model_g1 = Sequential()\n",
    "        model_g1.add(Input(shape=(3,)))\n",
    "        model_g1.add(Dense(10, activation='relu'))\n",
    "        model_g1.add(Dropout(0.2))\n",
    "        model_g1.add(Dense(1))\n",
    "        model_g1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model_g = [model_g0, model_g1]\n",
    "        for d in [0, 1]:\n",
    "            model_g[d].fit(x_train[d_train==d], y_train[d_train==d], epochs=35, batch_size=8, verbose=0)#\n",
    "            g_0_hat.append(model_g[d].predict(x_eval))\n",
    "\n",
    "        # Estimate propensity score m_0\n",
    "        model_m = Sequential()\n",
    "        model_m.add(Input(shape=(3,)))\n",
    "        model_m.add(Dense(5, activation='relu'))\n",
    "        model_m.add(Dropout(0.2))\n",
    "        model_m.add(Dense(1, activation='sigmoid'))\n",
    "        model_m.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        model_m.fit(x_train, d_train, epochs=35, batch_size=8, verbose=0)#\n",
    "        m_0_hat = model_m.predict(x_eval)#\n",
    "            \n",
    "        # Compute auxiliary estimator\n",
    "        scores = g_0_hat[1] - g_0_hat[0] + d_eval*(y_eval-g_0_hat[1])/m_0_hat - (1-d_eval)*(y_eval-g_0_hat[0])/(1-m_0_hat)\n",
    "        theta_0_check_list.append(np.mean(scores))\n",
    "\n",
    "        # For variance estimation\n",
    "        if inference:\n",
    "            scores_list.append(scores)\n",
    "\n",
    "        # For regression & IPW estimators\n",
    "        if classical:\n",
    "            reg_check_list.append(np.mean(g_0_hat[1] - g_0_hat[0])) \n",
    "            ipw_check_list.append(np.mean(d_eval*y_eval/m_0_hat - (1-d_eval)*y_eval/(1-m_0_hat)))     \n",
    "\n",
    "    # Compute final estimator\n",
    "    theta_0_hat = np.mean(theta_0_check_list)\n",
    "    if classical:\n",
    "        reg_hat, ipw_hat = np.mean(reg_check_list), np.mean(ipw_check_list)\n",
    "\n",
    "    # Inference: estimate variance and construct confidence interval\n",
    "    if inference:\n",
    "        sigma_hat = np.sqrt(np.mean((np.array(scores_list)-theta_0_hat)**2))\n",
    "        quantile = norm.ppf(1-alpha/2)\n",
    "        CI = np.array([theta_0_hat-quantile*sigma_hat/np.sqrt(N), theta_0_hat+quantile*sigma_hat/np.sqrt(N)])\n",
    "\n",
    "    # Return results\n",
    "    if classical:\n",
    "        if inference:\n",
    "            return np.array([theta_0_hat, reg_hat, ipw_hat]), sigma_hat, CI\n",
    "        else:\n",
    "            return np.array([theta_0_hat, reg_hat, ipw_hat])\n",
    "    else:\n",
    "        if inference:\n",
    "            return theta_0_hat, sigma_hat, CI\n",
    "        else:\n",
    "            return theta_0_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cafa473-563b-459d-ba4c-e58866a6849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7655ca-4824-4216-9eaf-62558826423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "N = 2000\n",
    "y_data, d_data, x_data = get_data(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02662de-4df8-4cae-9161-33e5979bd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.7985 - val_loss: 0.7331\n",
      "Epoch 2/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7816 - val_loss: 0.7026\n",
      "Epoch 3/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7442 - val_loss: 0.6804\n",
      "Epoch 4/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6916 - val_loss: 0.6590\n",
      "Epoch 5/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7255 - val_loss: 0.6419\n",
      "Epoch 6/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6255 - val_loss: 0.6155\n",
      "Epoch 7/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6840 - val_loss: 0.5996\n",
      "Epoch 8/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5987 - val_loss: 0.5796\n",
      "Epoch 9/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6136 - val_loss: 0.5584\n",
      "Epoch 10/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5554 - val_loss: 0.5349\n",
      "Epoch 11/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5888 - val_loss: 0.5181\n",
      "Epoch 12/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5362 - val_loss: 0.5025\n",
      "Epoch 13/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5729 - val_loss: 0.4909\n",
      "Epoch 14/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5706 - val_loss: 0.4779\n",
      "Epoch 15/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5969 - val_loss: 0.4676\n",
      "Epoch 16/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5866 - val_loss: 0.4583\n",
      "Epoch 17/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5042 - val_loss: 0.4485\n",
      "Epoch 18/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5389 - val_loss: 0.4408\n",
      "Epoch 19/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4825 - val_loss: 0.4296\n",
      "Epoch 20/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5080 - val_loss: 0.4201\n",
      "Epoch 21/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5056 - val_loss: 0.4137\n",
      "Epoch 22/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5333 - val_loss: 0.4083\n",
      "Epoch 23/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4829 - val_loss: 0.4043\n",
      "Epoch 24/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4906 - val_loss: 0.4004\n",
      "Epoch 25/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5223 - val_loss: 0.3962\n",
      "Epoch 26/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4519 - val_loss: 0.3909\n",
      "Epoch 27/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4574 - val_loss: 0.3916\n",
      "Epoch 28/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4612 - val_loss: 0.3913\n",
      "Epoch 29/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4843 - val_loss: 0.3842\n",
      "Epoch 30/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4755 - val_loss: 0.3834\n",
      "Epoch 31/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4272 - val_loss: 0.3818\n",
      "Epoch 32/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4968 - val_loss: 0.3814\n",
      "Epoch 33/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4386 - val_loss: 0.3844\n",
      "Epoch 34/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4844 - val_loss: 0.3758\n",
      "Epoch 35/35\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4497 - val_loss: 0.3769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13b2acafbd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_g0 = Sequential()\n",
    "model_g0.add(Input(shape=(3,)))\n",
    "model_g0.add(Dense(10, activation='relu'))\n",
    "model_g0.add(Dropout(0.2))\n",
    "model_g0.add(Dense(1))\n",
    "model_g0.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_g0.fit(x_data[d_data==0], y_data[d_data==0], epochs=35, batch_size=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5398495e-4309-43f3-8172-9cec57f4efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4194 - val_loss: 0.8790\n",
      "Epoch 2/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8454 - val_loss: 0.6625\n",
      "Epoch 3/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7367 - val_loss: 0.5882\n",
      "Epoch 4/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8371 - val_loss: 0.5357\n",
      "Epoch 5/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6814 - val_loss: 0.5239\n",
      "Epoch 6/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5479 - val_loss: 0.5173\n",
      "Epoch 7/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6155 - val_loss: 0.4979\n",
      "Epoch 8/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5799 - val_loss: 0.4958\n",
      "Epoch 9/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5576 - val_loss: 0.4913\n",
      "Epoch 10/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5569 - val_loss: 0.4821\n",
      "Epoch 11/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5106 - val_loss: 0.4946\n",
      "Epoch 12/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5393 - val_loss: 0.4680\n",
      "Epoch 13/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6045 - val_loss: 0.4722\n",
      "Epoch 14/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5852 - val_loss: 0.4623\n",
      "Epoch 15/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5062 - val_loss: 0.4660\n",
      "Epoch 16/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5705 - val_loss: 0.4590\n",
      "Epoch 17/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4923 - val_loss: 0.4574\n",
      "Epoch 18/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5064 - val_loss: 0.4654\n",
      "Epoch 19/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4539 - val_loss: 0.4553\n",
      "Epoch 20/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4446 - val_loss: 0.4657\n",
      "Epoch 21/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5700 - val_loss: 0.4463\n",
      "Epoch 22/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5176 - val_loss: 0.4453\n",
      "Epoch 23/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4838 - val_loss: 0.4435\n",
      "Epoch 24/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4839 - val_loss: 0.4513\n",
      "Epoch 25/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4725 - val_loss: 0.4471\n",
      "Epoch 26/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4351 - val_loss: 0.4420\n",
      "Epoch 27/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4717 - val_loss: 0.4480\n",
      "Epoch 28/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5142 - val_loss: 0.4418\n",
      "Epoch 29/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4257 - val_loss: 0.4478\n",
      "Epoch 30/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4288 - val_loss: 0.4392\n",
      "Epoch 31/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4586 - val_loss: 0.4432\n",
      "Epoch 32/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4191 - val_loss: 0.4395\n",
      "Epoch 33/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4588 - val_loss: 0.4482\n",
      "Epoch 34/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4436 - val_loss: 0.4417\n",
      "Epoch 35/35\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4427 - val_loss: 0.4442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13b2e247c50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_g1 = Sequential()\n",
    "model_g1.add(Input(shape=(3,)))\n",
    "model_g1.add(Dense(10, activation='relu'))\n",
    "model_g1.add(Dropout(0.2))\n",
    "model_g1.add(Dense(1))\n",
    "model_g1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_g1.fit(x_data[d_data==1], y_data[d_data==1], epochs=35, batch_size=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea251baf-b84d-4124-95f8-ad655b2a76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g0 = Sequential()\n",
    "model_g0.add(Input(shape=(3,)))\n",
    "model_g0.add(Dense(10, activation='relu'))\n",
    "model_g0.add(Dropout(0.2))\n",
    "model_g0.add(Dense(1))\n",
    "model_g0.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model_g1 = Sequential()\n",
    "model_g1.add(Input(shape=(3,)))\n",
    "model_g1.add(Dense(10, activation='relu'))\n",
    "model_g1.add(Dropout(0.2))\n",
    "model_g1.add(Dense(1))\n",
    "model_g1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model_g = [model_g0, model_g1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5eede499-acc7-4a26-afb7-d926da54af0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8400 - val_loss: 0.6688\n",
      "Epoch 2/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6476 - val_loss: 0.5864\n",
      "Epoch 3/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6115 - val_loss: 0.5623\n",
      "Epoch 4/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5853 - val_loss: 0.5500\n",
      "Epoch 5/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5958 - val_loss: 0.5443\n",
      "Epoch 6/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5852 - val_loss: 0.5415\n",
      "Epoch 7/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5899 - val_loss: 0.5388\n",
      "Epoch 8/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5827 - val_loss: 0.5351\n",
      "Epoch 9/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5723 - val_loss: 0.5326\n",
      "Epoch 10/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5642 - val_loss: 0.5312\n",
      "Epoch 11/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5608 - val_loss: 0.5292\n",
      "Epoch 12/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5749 - val_loss: 0.5264\n",
      "Epoch 13/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5757 - val_loss: 0.5262\n",
      "Epoch 14/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5664 - val_loss: 0.5258\n",
      "Epoch 15/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5737 - val_loss: 0.5244\n",
      "Epoch 16/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5567 - val_loss: 0.5231\n",
      "Epoch 17/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5945 - val_loss: 0.5243\n",
      "Epoch 18/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5500 - val_loss: 0.5211\n",
      "Epoch 19/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5536 - val_loss: 0.5217\n",
      "Epoch 20/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5698 - val_loss: 0.5202\n",
      "Epoch 21/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5796 - val_loss: 0.5195\n",
      "Epoch 22/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5461 - val_loss: 0.5193\n",
      "Epoch 23/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5480 - val_loss: 0.5199\n",
      "Epoch 24/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5520 - val_loss: 0.5197\n",
      "Epoch 25/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5546 - val_loss: 0.5192\n",
      "Epoch 26/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5615 - val_loss: 0.5188\n",
      "Epoch 27/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5576 - val_loss: 0.5187\n",
      "Epoch 28/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5633 - val_loss: 0.5184\n",
      "Epoch 29/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5565 - val_loss: 0.5183\n",
      "Epoch 30/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5625 - val_loss: 0.5178\n",
      "Epoch 31/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5595 - val_loss: 0.5176\n",
      "Epoch 32/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5360 - val_loss: 0.5169\n",
      "Epoch 33/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5450 - val_loss: 0.5175\n",
      "Epoch 34/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5468 - val_loss: 0.5165\n",
      "Epoch 35/35\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5356 - val_loss: 0.5167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13b3d38e850>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m = Sequential()\n",
    "model_m.add(Input(shape=(3,)))\n",
    "model_m.add(Dense(5, activation='relu'))\n",
    "model_m.add(Dropout(0.2))\n",
    "model_m.add(Dense(1, activation='sigmoid'))\n",
    "model_m.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model_m.fit(x_data, d_data, epochs=35, batch_size=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7350c6a9-9833-4e1b-abb8-c430f55d3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m = Sequential()\n",
    "model_m.add(Input(shape=(3,)))\n",
    "model_m.add(Dense(5, activation='relu'))\n",
    "model_m.add(Dropout(0.2))\n",
    "model_m.add(Dense(1, activation='sigmoid'))\n",
    "model_m.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecb9ae2a-afee-4190-85db-cee248ddcc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.7701746 , 0.78597087, 0.8578129 , 0.9682587 , 0.44352666,\n",
       "       0.78307426, 0.9342824 , 0.6365525 , 0.6002639 , 0.7358402 ,\n",
       "       0.14644738, 0.78670967, 0.25117138, 0.7478894 , 0.34183916],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.predict(x_data)[-15:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74dbdc10-ee36-4097-8fe7-e85fe628d47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78148794, 0.80765035, 0.88396065, 0.97975005, 0.42626086,\n",
       "       0.80256674, 0.95173849, 0.62505032, 0.60677442, 0.74982268,\n",
       "       0.10918031, 0.81506017, 0.21515688, 0.77405298, 0.33681074])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_data, d_data)\n",
    "logreg.predict_proba(x_data)[-15:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "15429a6a-cc93-4584-809e-86d935ff24f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80961937, 0.80817676, 0.91257872, 0.98248129, 0.4052997 ,\n",
       "       0.81438695, 0.95669844, 0.61823919, 0.60095135, 0.72017885,\n",
       "       0.09434526, 0.8405147 , 0.23682696, 0.76380239, 0.38884595])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_0(x_data)[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d761c7fa-0639-4e55-8494-f3bae4d547ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8055433 , 0.8412702 , 0.87107456, 0.9424594 , 0.36030835,\n",
       "       0.7720746 , 0.9265446 , 0.61318547, 0.71301585, 0.7792926 ,\n",
       "       0.07776507, 0.8081694 , 0.27150142, 0.7862124 , 0.27370057],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(objective='binary:logistic', max_depth=3, subsample=0.8, learning_rate=0.06, reg_lambda=10)\n",
    "model_xgb.fit(x_data, d_data)\n",
    "model_xgb.predict_proba(x_data)[-15:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ea93f2-2a38-4f1d-b945-24916a38baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "CPU times: total: 28.6 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ate_estimates, sigma_hat, CI = dml_ate(y_data, d_data, x_data, model_g, model_m, K=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a1fb235-d598-47c6-ab35-eebac80e4a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00078   , 1.08489418, 0.99473899])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9ee10d9-f4c4-4073-9ac3-87c2bbecc947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.078498 , 2.6367546])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d61cbf-2f27-411a-95e3-ba262c9aea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "for N, xgb_params_dict in xgb_params_dict_dict.items():\n",
    "    model_g0, model_g1 = xgb.XGBRegressor(objective='reg:squarederror'), xgb.XGBRegressor(objective='reg:squarederror')\n",
    "    model_g0.set_params(**xgb_params_dict['g0'])\n",
    "    model_g1.set_params(**xgb_params_dict['g1'])\n",
    "    model_g = [model_g0, model_g1]\n",
    "    model_m = xgb.XGBClassifier(objective='binary:logistic')\n",
    "    model_m.set_params(**xgb_params_dict['m'])\n",
    "    results_dict[N] = mc_simulation(N, model_g, model_m)\n",
    "    print(f'MC simulation done for N={N}')\n",
    "\n",
    "with open('results_dict.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(results_dict, pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
